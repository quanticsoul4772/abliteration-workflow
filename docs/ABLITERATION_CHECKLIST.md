# Abliteration Checklist

**Purpose:** A reusable checklist for every abliteration run to prevent mistakes and ensure consistency.

**Usage:** Copy this checklist for each new model. Check off items as you complete them.

---

## üî¥ BEFORE I DO ANYTHING

### Pre-Session Checks (Must complete before ANY action)

```
‚ñ° 1. Read CLAUDE.md completely (not skimmed)
‚ñ° 2. Read WORKFLOW.md for bruno-vast commands
‚ñ° 3. Understand what the user ACTUALLY wants
‚ñ° 4. Check what already exists (local files, previous runs, downloaded models)
‚ñ° 5. Verify local hardware can't do this first (RTX 4070, 8GB VRAM)
‚ñ° 6. Confirm cloud GPU is actually necessary
```

### Permission Checks (I will NOT proceed without explicit "yes")

| Action | Ask Permission? | Why |
|--------|----------------|-----|
| Create GPU instance | ‚úÖ YES | Costs money |
| Run any destructive command (pkill, rm, stop) | ‚úÖ YES | Irreversible |
| Start abliteration (200 trials) | ‚úÖ YES | Commits to 6+ hours |
| Disable any feature | ‚úÖ YES | Ruined Model 1 |
| Upload to HuggingFace | ‚úÖ YES | Public action |
| Stop/terminate instance | ‚úÖ YES | Kills processes |

---

## üü° MODEL PREPARATION

### Model Research Checklist

```
‚ñ° Model name: ____________________
‚ñ° Architecture type: ‚ñ° Dense  ‚ñ° MoE  ‚ñ° Multimodal
‚ñ° Total parameters: ______ B
‚ñ° Active parameters (if MoE): ______ B
‚ñ° Required transformers version: ______
‚ñ° trust_remote_code required: ‚ñ° Yes  ‚ñ° No
‚ñ° Gated model (requires HF auth): ‚ñ° Yes  ‚ñ° No
‚ñ° Context length: ______ tokens
```

### MoE Compatibility Check (if MoE architecture)

Bruno supports these MoE patterns:
```
‚ñ° layer.mlp.experts (Qwen3 style)
‚ñ° layer.block_sparse_moe.experts (Phi-MoE style)
‚ñ° layer.moe.experts (Granite MoE style)
‚ñ° layer.feed_forward.experts (gpt-oss style)
```

**If model uses different pattern:** May need bruno modification. Test with 2 trials first!

---

## üü¢ RESOURCE CALCULATION

### VRAM Requirements

| Precision | Formula | My Model |
|-----------|---------|----------|
| BF16/FP16 | params √ó 2 bytes | ______ GB |
| 8-bit | params √ó 1 byte | ______ GB |
| 4-bit | params √ó 0.5 bytes | ______ GB |

### Disk Requirements

| Component | Size | My Model |
|-----------|------|----------|
| Model weights | params √ó 2 | ______ GB |
| Output model | params √ó 2 | ______ GB |
| HuggingFace cache | ~5GB | 5 GB |
| Optuna database | ~1GB | 1 GB |
| C4 dataset (streaming) | ~0GB | 0 GB |
| Working space | ~10GB | 10 GB |
| **MINIMUM TOTAL** | | ______ GB |
| **RECOMMENDED (+50%)** | | ______ GB |

### GPU Selection

| Model Size | Minimum GPU | Recommended GPU | Notes |
|------------|-------------|-----------------|-------|
| 7B | RTX 4090 (24GB) | A6000 (48GB) | |
| 13B | A100 40GB | A100 80GB | |
| 16B MoE | A100 40GB | A100 80GB | |
| 32B | H100 80GB | **H200 141GB** | ‚ö†Ô∏è See warning below |
| 70B | H200 141GB | 2x H100 | |

**‚ö†Ô∏è CRITICAL: H100 80GB vs H200 141GB for 32B Models**

| GPU | VRAM | 32B Model + Cache | Result |
|-----|------|-------------------|--------|
| H100 80GB | 80GB | ~92GB needed | ‚ùå **OOM - must use --cache-weights false** |
| H200 141GB | 141GB | ~92GB needed | ‚úÖ **Works with caching (recommended)** |

**If using H100 80GB for 32B:** Must disable caching (`--cache-weights false`), which adds 3-4 hours to training time.

**My selection:** ____________________

---

## üîµ INSTANCE CREATION

### Search Commands

```bash
# Check availability (run these, report results, wait for permission)
vastai search offers "gpu_name=<GPU> disk_space>=<DISK> rentable=true" --order dph_total
```

### Before Creating Instance

```
‚ñ° Searched for available GPUs
‚ñ° Reported options to user with prices
‚ñ° Received explicit permission to create
‚ñ° Disk size calculated: ______ GB
‚ñ° GPU tier confirmed: ______
```

### Instance Creation Command

**Option 1: Use Bruno Abliteration Template (RECOMMENDED)**
```bash
# Uses our custom template with bruno pre-installed
# Template ID: 337981
# Template Hash: 7ea2682501dc881b1097c18f096f7c63
vastai launch instance -g A100_SXM4 -n 1 --template_hash 7ea2682501dc881b1097c18f096f7c63 -d 150

# Or create from offer ID:
vastai create instance <OFFER_ID> --template_hash 7ea2682501dc881b1097c18f096f7c63 --disk 150
```

**Option 2: Use Vast.ai PyTorch Template (Fallback)**
```bash
# Uses the official vastai/pytorch template - requires manual bruno install
vastai launch instance -g A100_SXM4 -n 1 -i vastai/pytorch -d 150 --ssh --direct
```

**Option 3: Direct instance creation (Manual)**
```bash
# ONLY RUN AFTER USER SAYS "YES"
vastai create instance <OFFER_ID> \
  --disk <DISK_SIZE> \
  --image pytorch/pytorch:2.4.0-cuda12.4-cudnn9-devel \
  --onstart-cmd "pip install huggingface_hub accelerate && pip install git+https://github.com/quanticsoul4772/bruno.git"
```

**Bruno Abliteration Template (ID: 337981) Benefits:**
- ‚úÖ Bruno pre-installed on startup
- ‚úÖ HuggingFace Hub + Accelerate included
- ‚úÖ `HF_HOME` and `HF_TRUST_REMOTE_CODE` pre-configured
- ‚úÖ SSH + Direct connection enabled
- ‚úÖ 40GB+ VRAM, 150GB+ disk, CUDA 12.1+ filters
- ‚úÖ Public template - works for anyone

### After Creation - BE PATIENT!

**‚ö†Ô∏è PATIENCE RULES:**
- Instance startup can take **5-10 minutes** (not 1-2 minutes)
- SSH may not be available immediately even if instance shows "running"
- Network connectivity varies - don't assume failure too quickly
- **DO NOT terminate and recreate** - just wait longer
- If SSH fails, wait 2 more minutes and try again
- Only consider recreating after 15+ minutes of no connectivity

**Startup Timeline (typical):**
| Phase | Time | What's Happening |
|-------|------|------------------|
| Instance creation | 0-1 min | Vast.ai allocating resources |
| Container pulling | 1-5 min | Docker image downloading |
| onstart script | 2-5 min | pip install, setup commands |
| SSH ready | 5-10 min | Full connectivity available |

```
‚ñ° Created instance - note the time: ______
‚ñ° Waited at least 5 minutes before first SSH attempt
‚ñ° If SSH fails, waited 2 more minutes and retried
‚ñ° Verified instance is running: uv run bruno-vast list
‚ñ° Verified disk space: uv run bruno-vast exec "df -h /workspace"
‚ñ° Verified bruno installed: uv run bruno-vast exec "which bruno"
‚ñ° If still failing after 15 minutes, THEN consider recreating
```

---

## üü£ ABLITERATION EXECUTION

### Environment Setup

```
‚ñ° export HF_HOME=/workspace/.cache/huggingface
‚ñ° export HF_TOKEN=hf_...
‚ñ° export HF_TRUST_REMOTE_CODE=1  (for models with custom code)
```

### ‚ö†Ô∏è COMPATIBILITY TEST (REQUIRED)

**ALWAYS run 2-trial test before committing to 200 trials!**

```bash
# Run in tmux to survive SSH disconnect
tmux new-session -d -s bruno-test '
export HF_HOME=/workspace/.cache/huggingface
export HF_TRUST_REMOTE_CODE=1
bruno \
  --model <MODEL> \
  --auto-select true \
  --auto-select-path /workspace/models \
  --storage sqlite:////workspace/bruno_test.db \
  --study-name test-2trials \
  --n-trials 2 \
  --cache-weights true \
  --unhelpfulness-prompts.dataset allenai/c4 \
  --unhelpfulness-prompts.config en \
  --unhelpfulness-prompts.split "train[:200]" \
  --unhelpfulness-prompts.column text \
  2>&1 | tee /workspace/bruno_test.log
'
```

### Test Verification Checklist

```
‚ñ° Model loads without errors
‚ñ° "Extracting refusal directions" completes
‚ñ° "Trial 0 finished" appears in logs
‚ñ° "Trial 1 finished" appears in logs
‚ñ° No KeyError or AttributeError for MoE layers
‚ñ° MoE experts detected (if MoE model)
‚ñ° No OOM errors
```

**If test fails:** STOP. Report error to user. Do not try workarounds.

### Full Training (AFTER test passes, AFTER user permission)

```bash
# ONLY RUN AFTER:
# 1. Compatibility test passed
# 2. User explicitly approved

tmux new-session -d -s bruno '
export HF_HOME=/workspace/.cache/huggingface
export HF_TRUST_REMOTE_CODE=1
bruno \
  --model <MODEL> \
  --auto-select true \
  --auto-select-path /workspace/models \
  --storage sqlite:////workspace/bruno_study.db \
  --study-name <MODEL_NAME>-abliteration \
  --n-trials 200 \
  --cache-weights true \
  --unhelpfulness-prompts.dataset allenai/c4 \
  --unhelpfulness-prompts.config en \
  --unhelpfulness-prompts.split "train[:200]" \
  --unhelpfulness-prompts.column text \
  2>&1 | tee /workspace/bruno.log
'
```

### Monitoring Commands

```bash
# Live dashboard
uv run bruno-vast watch

# Check progress
uv run bruno-vast progress

# View logs
uv run bruno-vast exec "tail -100 /workspace/bruno.log"

# Attach to tmux session
uv run bruno-vast exec "tmux attach -t bruno"
# Detach: Ctrl+B then D
```

---

## üü§ POST-TRAINING

### Training Completion Checks

```
‚ñ° Logs show "Best trial" message
‚ñ° Logs show "Saved" message
‚ñ° Model exists: ls -la /workspace/models/
‚ñ° Model size correct: du -sh /workspace/models/*
‚ñ° No errors in last 100 lines of log
```

### Download Checklist

```
‚ñ° Verified training complete
‚ñ° Verified model exists on instance
‚ñ° Checked local disk space (need model size + buffer)
‚ñ° Download started: rsync or bruno-vast download
   ‚ö†Ô∏è Windows users: rsync requires WSL - run from WSL terminal, NOT PowerShell!
‚ñ° Download completed successfully
‚ñ° Verified local files match remote size
```

### Local Testing (BEFORE upload)

```
‚ñ° Model loads locally without errors
‚ñ° Tested with restricted prompt
‚ñ° Response is helpful (no refusal)
‚ñ° No excessive moralizing
‚ñ° No disclaimers or warnings
```

**If model still refuses:** STOP. Abliteration did not work. Do NOT upload.

---

## üîµ HUGGINGFACE UPLOAD

### Pre-Upload Fixes

```
‚ñ° Fixed tokenizer_config.json (extra_special_tokens: {} not [])
‚ñ° Created README.md model card
‚ñ° Created handler.py (if using inference endpoints)
‚ñ° Created requirements.txt
```

### Upload Checklist

```
‚ñ° Received permission to upload
‚ñ° Repository name confirmed with user
‚ñ° Uploaded successfully
‚ñ° Model card displays correctly
‚ñ° Files all present on HuggingFace
```

---

## üü† CLEANUP

### Before Stopping Instance

```
‚ñ° Verified model downloaded to local machine
‚ñ° Verified local model loads correctly
‚ñ° Received explicit permission to stop
‚ñ° Understand stopping KILLS any running process
```

### Cleanup Commands

```bash
# ONLY AFTER user confirms download complete
uv run bruno-vast stop

# Or terminate completely
uv run bruno-vast terminate <INSTANCE_ID>
```

---

## ‚õî THINGS I WILL NOT DO

| Action | Why |
|--------|-----|
| Disable `--orthogonalize-directions` | Ruined Model 1 - model still moralizes |
| Disable any feature without permission | User must explicitly approve |
| Use 100GB disk for 32B models | Caused data loss on Model 2 |
| Skip the 2-trial compatibility test | Could waste 6+ hours on broken config |
| Proceed without explicit "yes" | Multiple past mistakes from assuming |
| Try workarounds when errors occur | Made things worse every time |
| Run commands without tmux | SSH disconnect kills process |
| Upload before local testing | Could publish broken model |
| Use H100 80GB with --cache-weights true for 32B | OOM - 92GB needed > 80GB available |
| Terminate instance because SSH fails once | Instance may still be starting up |
| Assume connectivity failure after 2 minutes | Wait at least 10 minutes before giving up |
| Redeploy without waiting for first instance | Wastes money and time |

---

## üìä RUN LOG

Fill this in for each abliteration run:

```
Model: ____________________
Date Started: ____________________
GPU Used: ____________________
Vast.ai Instance ID: ____________________
Instance Cost/hr: $______

Compatibility Test:
  Started: ____________________
  Completed: ____________________
  Result: ‚ñ° Pass  ‚ñ° Fail

Full Training:
  Started: ____________________
  Completed: ____________________
  Total Trials: ______
  Best Trial: ______
  Final KL Divergence: ______
  Final Refusals: ______

Download:
  Started: ____________________
  Completed: ____________________
  Local Path: ____________________

Upload:
  Repository: ____________________
  Completed: ____________________

Total Cost: $______
Total Time: ______ hours

Notes:
____________________
____________________
____________________
```

---

## üìú ABLITERATION HISTORY

| # | Model | Date | Status | GPU | Cost | Notes |
|---|-------|------|--------|-----|------|-------|
| 1 | Qwen2.5-7B-Instruct | Feb 2026 | ‚ö†Ô∏è Partial | 4x RTX 4090 | ~$15 | orthogonalization disabled |
| 2 | Qwen2.5-Coder-32B-Instruct | Feb 2026 | ‚úÖ Success | H200 | ~$25 | Trial 173, 0 refusals |
| 3 | Moonlight-16B-A3B-Instruct | Feb 2026 | üîÑ Pending | TBD | TBD | First MoE model |

---

## üîß QUICK REFERENCE: CLI FLAGS

**Required for all runs:**
```bash
--model <MODEL>
--auto-select true                    # NOT just --auto-select
--auto-select-path /workspace/models
--storage sqlite:////workspace/bruno_study.db
--study-name <NAME>
```

**Required for C4 dataset (pass ALL four):**
```bash
--unhelpfulness-prompts.dataset allenai/c4
--unhelpfulness-prompts.config en
--unhelpfulness-prompts.split "train[:200]"
--unhelpfulness-prompts.column text
```

**Memory settings:**
```bash
--cache-weights true    # Default, works on H200 for 32B (v1.2.0+)
--cache-weights false   # Fallback for H100 80GB with 32B models
```

**Optional optimizations:**
```bash
--compile              # 1.5-2x inference speedup
--n-trials 200         # Default, adjust as needed
```

---

## üîß OOM TROUBLESHOOTING GUIDE

**CRITICAL: Diagnose software issues FIRST before blaming hardware!**

**Note:** All diagnostic commands below run on the **remote Vast.ai instance** via `bruno-vast exec`, not your local Windows machine.

When you encounter an OOM error, follow this diagnostic process in order:

### Step 1: Identify the EXACT Error

```bash
# Check the full error message
uv run bruno-vast exec "tail -200 /workspace/bruno.log | grep -A 10 -i 'error\|oom\|memory'"

# Check system OOM killer
uv run bruno-vast exec "dmesg | tail -50 | grep -i oom"
```

**Common OOM Error Types:**

| Error Message | Likely Cause | Software Fix First |
|---------------|--------------|--------------------|
| `torch.cuda.OutOfMemoryError` | GPU VRAM exhausted | Reduce batch_size, disable caching |
| `CUDA out of memory. Tried to allocate X GiB` | Single allocation too large | Reduce batch_size to 1 |
| `RuntimeError: out of memory` | General memory issue | Check for memory leaks |
| `Killed` (no error message) | System OOM killer | Check system RAM, not just GPU |
| `No space left on device` | **DISK**, not memory! | Clear cache, increase disk |
| `MemoryError` during download | System RAM exhausted | Set `HF_HOME` to disk with more space |

### Step 2: Check Current Memory State

```bash
# GPU memory breakdown
uv run bruno-vast exec "nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv"

# System RAM
uv run bruno-vast exec "free -h"

# Disk space (often confused with memory!)
uv run bruno-vast exec "df -h /workspace"

# Python process memory
uv run bruno-vast exec "ps aux --sort=-%mem | head -10"
```

### Step 3: Identify the Culprit Phase

**OOM can occur in different phases - each has different fixes:**

| Phase | When It Happens | Memory Usage | Fix |
|-------|-----------------|--------------|-----|
| **Model Loading** | First 1-2 minutes | Model size √ó 2 | Use `--cache-weights false` |
| **Batch Size Detection** | "Trying batch size X" | Batch √ó sequence length | Set explicit `--batch-size 4` |
| **Direction Extraction** | "Extracting refusal directions" | Residuals in memory | Reduce split size (e.g., `train[:100]`) |
| **Weight Caching** | "Caching abliterable weights" | +28GB for 32B model | Use `--cache-weights false` |
| **Trial Evaluation** | "Running trial X" | Generation memory | Reduce `--refusal-check-tokens` |
| **Model Saving** | "Saving model" | Full model copy | Clear cache first |

### Step 4: Software Fixes (Try These BEFORE Hardware)

**Fix 1: Disable weight caching (biggest memory saver)**
```bash
bruno --model MODEL --cache-weights false
# Saves ~28GB for 32B models, ~6GB for 7B models
```

**Fix 2: Reduce batch size**
```bash
bruno --model MODEL --batch-size 1 --max-batch-size 4
```

**Fix 3: Reduce token generation**
```bash
bruno --model MODEL --refusal-check-tokens 20
```

**Fix 4: Clear GPU cache between operations**
```python
# bruno already does this, but if you're debugging:
import torch; torch.cuda.empty_cache()
import gc; gc.collect()
```

**Fix 5: Check for memory leaks (multiple trials)**
```bash
# If OOM happens on trial 50 but not trial 1, suspect memory leak
uv run bruno-vast exec "watch -n 5 nvidia-smi"
# Bruno clears cache between trials, but leaks can occur.
# If you see gradual memory increase, report issue at GitHub.
```

**Fix 6: Multi-GPU memory imbalance**
```bash
# Check if GPU 0 is overloaded while others are empty
uv run bruno-vast exec "nvidia-smi"
# If GPU 0: 23GB, GPU 1-3: <1GB ‚Üí device_map issue
# Fix: Use device_map="balanced" not "auto" in config
```

### Step 5: Memory Calculations

**Before blaming GPU size, calculate expected usage:**

| Component | Formula | 7B Model | 32B Model |
|-----------|---------|----------|------------|
| Model weights (BF16) | params √ó 2 bytes | 14 GB | 64 GB |
| Layer-wise cache | ~43% of model | 6 GB | 28 GB |
| KV cache (per batch) | layers √ó heads √ó seq √ó 2 √ó 2 | 0.5 GB | 2 GB |
| Residuals (extraction) | prompts √ó layers √ó hidden √ó 4 | 2 GB | 8 GB |
| Working memory | ~10% of model | 1.5 GB | 6 GB |
| **TOTAL (with caching)** | | **24 GB** | **108 GB** |
| **TOTAL (without caching)** | | **18 GB** | **80 GB** |

**GPU Selection Based on Calculation:**

| Model Size | With Caching | Without Caching | Recommended GPU |
|------------|--------------|-----------------|------------------|
| 7B | 24 GB | 18 GB | RTX 4090 (24 GB) |
| 13B | 45 GB | 32 GB | A100 40GB |
| 32B | 108 GB | 80 GB | H200 (141 GB) |
| 70B | 200 GB+ | 160 GB | 2√ó H100 |

### Step 6: Bruno's Built-in OOM Recovery

Bruno has automatic OOM recovery (see `src/bruno/main.py`):

1. **Catches `torch.cuda.OutOfMemoryError`** during evaluation
2. **Clears GPU cache** with `empty_cache()`
3. **Reloads model** to reset state
4. **Reduces batch size** by half automatically
5. **Retries up to 3 times** (`MAX_OOM_RETRIES`)
6. **Preserves progress** via Optuna storage (resume supported)

**If bruno's recovery triggers:**
```
[red]GPU OOM detected (attempt 1/3)[/]
[yellow]GPU Memory: 138.5/141.0 GB used[/]
[yellow]Reducing batch_size to 2 and retrying...[/]
```

**This is EXPECTED behavior**, not a failure. Wait for it to recover.

### Step 7: When to ACTUALLY Blame Hardware

**Only blame hardware if ALL of these are true:**

1. ‚úÖ You've tried `--cache-weights false`
2. ‚úÖ You've tried `--batch-size 1`
3. ‚úÖ Memory calculation shows GPU is too small
4. ‚úÖ OOM happens on Trial 1 (not a leak)
5. ‚úÖ Model loads but fails on first operation

**Hardware upgrade needed when:**
- 32B model on H100 80GB with caching ‚Üí Need H200 141GB
- 70B model on single H200 ‚Üí Need 2√ó H100 or 8√ó A100

### Step 8: Diagnostic Commands Quick Reference

```bash
# Full diagnostic dump
uv run bruno-vast exec "
echo '=== GPU ===' && nvidia-smi
echo '=== RAM ===' && free -h
echo '=== Disk ===' && df -h /workspace
echo '=== Processes ===' && ps aux --sort=-%mem | head -5
echo '=== Python Memory ===' && python3 -c 'import torch; print(torch.cuda.memory_summary() if torch.cuda.is_available() else "No CUDA")'
"

# Memory over time (run in separate terminal)
uv run bruno-vast exec "while true; do nvidia-smi --query-gpu=timestamp,memory.used --format=csv,noheader; sleep 10; done"
```

---

## üìù OPERATOR PROMPTS (for user to paste)

### Session Start
```
You are helping me abliterate a model. Follow docs/ABLITERATION_CHECKLIST.md exactly.
Do NOT create instances, run commands, or disable features without explicit permission.
Stop and ask at checkpoints. Report errors and wait for instructions.
```

### Before Cloud Action
```
STOP. Before running this command:
1. What will this cost in time and money?
2. Is this reversible?
3. Have you confirmed prerequisites?
Wait for my "yes" before proceeding.
```

### When Things Go Wrong
```
STOP. Do not try to fix this on your own.
1. What exactly happened?
2. What do the logs say?
3. What are our options?
Wait for my decision.
```

### Emergency Stop
```
STOP. Do not spawn agents. Do not run commands. Just acknowledge.
```
